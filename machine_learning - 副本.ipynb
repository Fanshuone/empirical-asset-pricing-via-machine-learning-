{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463f2485",
   "metadata": {},
   "source": [
    "# Replication of Gu, Kelly and Xiu (2020, RFS)\n",
    "\n",
    "Current Version: Jan 2023\n",
    "\n",
    "By [Yanlin Bao](https://www.ylbao.dev/), PhD in Business (Finance) student at Singapore Management University.\n",
    "\n",
    "email: ylbao.2022@pbs.smu.edu.sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b58316",
   "metadata": {},
   "source": [
    "This jupyter notebook is a simple version of replication code in Python of the seminal work [Gu, Kelly and Xiu (2020)](https://doi.org/10.1093/rfs/hhaa009). In this version, I mainly focus on the implementation of the models used in this paper. I hope this can be a reference for researchers, students and practitioners who want to explore the machine learning application in asset pricing. I run this version on my laptop. Therefore, I only present the result of a subsample of the original dataset, and only focus on the top 1000 firm with respect to market capitalization. I also tried for the whole firm space and the bottom 1000 firms. The results are consistent with those in the paper that the stock return of large firms are more predictable. As a result, I tried some more agressive hyperparameter setting in validation process.\n",
    "\n",
    "Apart from the original RFS paper, for more details about the implementation, I refer the readers to the [online appendix](https://dachxiu.chicagobooth.edu/download/ML_supp.pdf) and [Q&A](https://www.dropbox.com/s/4vsc4hakwvz2j31/ML_QandA.pdf?dl=0) provided by the authors. Since the machine learning models have too many hyperparameters to choose, and the training process of some models have random initial guess for the parameters, fully replication of the results from the paper is impossible.\n",
    "\n",
    "Besides, I believe there are some mistakes in my code, and there are still some unsolved problems. Please feel free to play the code and contact me through email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09b44eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:57:49.230344Z",
     "start_time": "2024-06-17T08:57:47.611381Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc # garbage collection module to release memory usage in time\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# os.chdir('/Users/baoyanlin/Replication_Code/GKX_2020_RFS/data')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519bd7c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Firm Characteristics Data\n",
    "\n",
    "The newest version of firm characteristics data are downloaded from Prof Dacheng Xiu's [website](https://dachxiu.chicagobooth.edu/). The data may take 4 to 6 hours to download though. Prof [Yuan Yao](https://yao-lab.github.io/) provide a dropbox [link](https://www.dropbox.com/s/zzgjdubvv23xkfp/datashare.zip?dl=0) which takes only a few minutes to download. However, the current version ends at 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6c9f30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:59:37.914902Z",
     "start_time": "2024-06-17T08:57:49.234960Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27.7 s\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>RET</th>\n",
       "      <th>prc</th>\n",
       "      <th>SHROUT</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>baspread</th>\n",
       "      <th>ill</th>\n",
       "      <th>maxret</th>\n",
       "      <th>retvol</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>sic2</th>\n",
       "      <th>bm</th>\n",
       "      <th>bm_ia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>24355.500</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>9.8750</td>\n",
       "      <td>2498</td>\n",
       "      <td>0.037079</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.281788</td>\n",
       "      <td>8.395576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>1.098587e-06</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.972710</td>\n",
       "      <td>0.426715</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.868139</td>\n",
       "      <td>-0.104925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>78332.625</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>8526</td>\n",
       "      <td>0.206346</td>\n",
       "      <td>0.042579</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>8.067022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>6.509871e-06</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.054790</td>\n",
       "      <td>1.368962</td>\n",
       "      <td>0.759666</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.680296</td>\n",
       "      <td>-0.152630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>39836.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>20897</td>\n",
       "      <td>2.470629</td>\n",
       "      <td>6.104008</td>\n",
       "      <td>-1.170178</td>\n",
       "      <td>11.360419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138777</td>\n",
       "      <td>9.482216e-08</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.075671</td>\n",
       "      <td>0.465917</td>\n",
       "      <td>7.007556</td>\n",
       "      <td>8.756593e-09</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.061049</td>\n",
       "      <td>-0.397365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10016</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>379569.500</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>23.0625</td>\n",
       "      <td>16964</td>\n",
       "      <td>0.449866</td>\n",
       "      <td>0.202379</td>\n",
       "      <td>0.391222</td>\n",
       "      <td>12.024414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>5.643552e-08</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.040708</td>\n",
       "      <td>1.242227</td>\n",
       "      <td>8.102766</td>\n",
       "      <td>1.833562e-08</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>-0.227582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10019</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>28945.000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.7500</td>\n",
       "      <td>8270</td>\n",
       "      <td>2.249729</td>\n",
       "      <td>5.061279</td>\n",
       "      <td>0.203106</td>\n",
       "      <td>9.294773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131620</td>\n",
       "      <td>3.206363e-07</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.120324</td>\n",
       "      <td>0.983488</td>\n",
       "      <td>16.163956</td>\n",
       "      <td>7.497863e-09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.552262</td>\n",
       "      <td>0.036872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   permno       DATE       mvel1       RET      prc  SHROUT      beta  \\\n",
       "0   10001 2001-01-31   24355.500  0.012821   9.8750    2498  0.037079   \n",
       "1   10002 2001-01-31   78332.625  0.088435  10.0000    8526  0.206346   \n",
       "2   10012 2001-01-31   39836.000  0.500000   3.0000   20897  2.470629   \n",
       "3   10016 2001-01-31  379569.500  0.030726  23.0625   16964  0.449866   \n",
       "4   10019 2001-01-31   28945.000  0.071429   3.7500    8270  2.249729   \n",
       "\n",
       "     betasq     chmom     dolvol  ...  baspread           ill    maxret  \\\n",
       "0  0.001375  0.281788   8.395576  ...  0.020711  1.098587e-06  0.027778   \n",
       "1  0.042579  0.050021   8.067022  ...  0.033991  6.509871e-06  0.134328   \n",
       "2  6.104008 -1.170178  11.360419  ...  0.138777  9.482216e-08  0.129412   \n",
       "3  0.202379  0.391222  12.024414  ...  0.054578  5.643552e-08  0.070769   \n",
       "4  5.061279  0.203106   9.294773  ...  0.131620  3.206363e-07  0.435897   \n",
       "\n",
       "     retvol  std_dolvol   std_turn     zerotrade  sic2        bm     bm_ia  \n",
       "0  0.017710    0.972710   0.426715  4.200000e+00  49.0  0.868139 -0.104925  \n",
       "1  0.054790    1.368962   0.759666  4.200000e+00  60.0  0.680296 -0.152630  \n",
       "2  0.075671    0.465917   7.007556  8.756593e-09  36.0  0.061049 -0.397365  \n",
       "3  0.040708    1.242227   8.102766  1.833562e-08  38.0  0.287808 -0.227582  \n",
       "4  0.120324    0.983488  16.163956  7.497863e-09  38.0  0.552262  0.036872  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# start date and end date of the sample\n",
    "#stdt, nddt = 19570101, 20161231\n",
    "stdt, nddt = 20010101, 20201231\n",
    "\n",
    "# load firm characteristics data\n",
    "data_ch = pd.read_csv('../GKX_20201231.csv')\n",
    "data_ch = data_ch[(data_ch['DATE']>=stdt)&(data_ch['DATE']<=nddt)].reset_index(drop=True)\n",
    "data_ch['DATE'] = pd.to_datetime(data_ch['DATE'],format='%Y%m%d')+pd.offsets.MonthEnd(0)\n",
    "characteristics = list(set(data_ch.columns).difference({'permno','DATE','SHROUT','mve0','sic2','RET','prc'}))\n",
    "\n",
    "data_ch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05873f",
   "metadata": {},
   "source": [
    "### Pick out Top 1000 and Bottom 1000 Firms\n",
    "\n",
    "Next, let's pick out the top 1000 and bottom 1000 firms with respect to market capitalization to see the differnce of predictability between big firms and small firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6774130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:59:43.777562Z",
     "start_time": "2024-06-17T08:59:37.930529Z"
    }
   },
   "outputs": [],
   "source": [
    "#mvell size\n",
    "data_ch_top = data_ch.sort_values('mvel1',ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "data_ch_bot = data_ch.sort_values('mvel1',ascending=False).groupby('DATE').tail(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb154c2",
   "metadata": {},
   "source": [
    "### Missing Characteristics\n",
    "According to the paper, the __missing data__ are replaced by the _cross-sectional median_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa34628a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:59:44.108622Z",
     "start_time": "2024-06-17T08:59:43.783545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permno            0\n",
       "DATE              0\n",
       "mvel1           276\n",
       "RET               0\n",
       "prc            7830\n",
       "              ...  \n",
       "std_turn        410\n",
       "zerotrade       342\n",
       "sic2          22532\n",
       "bm           375825\n",
       "bm_ia        375825\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing data before filling\n",
    "data_ch.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8350b08f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:02.995117Z",
     "start_time": "2024-06-17T08:59:44.111559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.6 s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill na with cross-sectional median\n",
    "for ch in characteristics:\n",
    "     data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14cc7d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:03.010551Z",
     "start_time": "2024-06-17T09:00:02.997076Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.Series.transform()\n",
    "# pd.DataFrame.transform()\n",
    "# data_ch.groupby('DATE')[['DATE','mvel1']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a9e918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:03.310521Z",
     "start_time": "2024-06-17T09:00:03.014095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permno            0\n",
       "DATE              0\n",
       "mvel1             0\n",
       "RET               0\n",
       "prc            7830\n",
       "              ...  \n",
       "std_turn          0\n",
       "zerotrade         0\n",
       "sic2          22532\n",
       "bm           104764\n",
       "bm_ia        104764\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing data after filling\n",
    "data_ch.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2306eb7",
   "metadata": {},
   "source": [
    "Since there are some characeristics that are all missing at some time point, we still encounter missing data after the filling process. Then, let's try to fill the remaining na with time-series median. Unfortunately, after filling na with time-series median, na still exists. Since there is no further instruction of how to deal with remaining na in the data, after consulting some replication code online, I __fill the remaining na with 0__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c11dba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:04.527305Z",
     "start_time": "2024-06-17T09:00:03.313862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prc', 'mve0', 'sic2'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ch in characteristics:\n",
    "     data_ch[ch] = data_ch[ch].fillna(0)\n",
    "    \n",
    "data_ch.columns[data_ch.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916e289",
   "metadata": {},
   "source": [
    "Now, we do not have missing characteristics in our dataset. Then, do the same process to top and bottom 1000 firms data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3803f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:04.543433Z",
     "start_time": "2024-06-17T09:00:04.529068Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_na(data_ch, characteristics):\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch[ch].fillna(0)\n",
    "    return data_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad5e236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:28.222844Z",
     "start_time": "2024-06-17T09:00:04.549062Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ch_top = fill_na(data_ch_top, characteristics)\n",
    "data_ch_bot = fill_na(data_ch_bot, characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcef889",
   "metadata": {},
   "source": [
    "### Transform SIC Code into Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b86c7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:28.238635Z",
     "start_time": "2024-06-17T09:00:28.225847Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dummies for SIC code\n",
    "def get_sic_dummies(data_ch):\n",
    "    sic_dummies = pd.get_dummies(data_ch['sic2'].fillna(999).astype(int),prefix='sic').drop('sic_999',axis=1)\n",
    "    data_ch_d = pd.concat([data_ch,sic_dummies],axis=1)\n",
    "    data_ch_d.drop(['prc','SHROUT','mve0','sic2'],inplace=True,axis=1)\n",
    "    return data_ch_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb13ea8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:31.170834Z",
     "start_time": "2024-06-17T09:00:28.242814Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ch_d = get_sic_dummies(data_ch)\n",
    "data_ch_top_d = get_sic_dummies(data_ch_top)\n",
    "data_ch_bot_d = get_sic_dummies(data_ch_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cd875",
   "metadata": {},
   "source": [
    "### Macroeconomic Predictors Data\n",
    "\n",
    "The eight macroeconomic predictors follows the definitions by Welch and Goyal (2008, RFS). The data are available on Prof Goyal's [website](https://sites.google.com/view/agoyal145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44ddfa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:31.279885Z",
     "start_time": "2024-06-17T09:00:31.173782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>dp_sp</th>\n",
       "      <th>ep_sp</th>\n",
       "      <th>bm_sp</th>\n",
       "      <th>ntis</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.035490</td>\n",
       "      <td>0.150450</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.004941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-28</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.156070</td>\n",
       "      <td>-0.006856</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.002528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.039161</td>\n",
       "      <td>0.133114</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.007140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>0.122497</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.007426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-31</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.120510</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      yyyymm     dp_sp     ep_sp     bm_sp      ntis     tbl     tms     dfy  \\\n",
       "0 2001-01-31  0.011839  0.035490  0.150450 -0.003193  0.0515  0.0047  0.0078   \n",
       "1 2001-02-28  0.012962  0.037873  0.156070 -0.006856  0.0488  0.0061  0.0077   \n",
       "2 2001-03-31  0.013766  0.039161  0.133114 -0.005213  0.0442  0.0117  0.0086   \n",
       "3 2001-04-30  0.012707  0.034060  0.122497 -0.002543  0.0387  0.0206  0.0087   \n",
       "4 2001-05-31  0.012567  0.031592  0.120510 -0.000248  0.0362  0.0232  0.0078   \n",
       "\n",
       "       svar  \n",
       "0  0.004941  \n",
       "1  0.002528  \n",
       "2  0.007140  \n",
       "3  0.007426  \n",
       "4  0.002536  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load macroeconomic predictors data\n",
    "data_ma = pd.read_csv('../PredictorData2023.xlsx - Monthly.csv')\n",
    "data_ma = data_ma[(data_ma['yyyymm']>=stdt//100)&(data_ma['yyyymm']<=nddt//100)].reset_index(drop=True)\n",
    "\n",
    "# construct predictor\n",
    "ma_predictors = ['dp_sp','ep_sp','bm_sp','ntis','tbl','tms','dfy','svar']\n",
    "data_ma['Index'] = data_ma['Index'].str.replace(',','').astype('float64')\n",
    "data_ma['dp_sp'] = data_ma['D12']/data_ma['Index']\n",
    "data_ma['ep_sp'] = data_ma['E12']/data_ma['Index']\n",
    "data_ma.rename({'b/m':'bm_sp'},axis=1,inplace=True)\n",
    "data_ma['tms'] = data_ma['lty']-data_ma['tbl']\n",
    "data_ma['dfy'] = data_ma['BAA']-data_ma['AAA']\n",
    "data_ma = data_ma[['yyyymm']+ma_predictors]\n",
    "data_ma['yyyymm'] = pd.to_datetime(data_ma['yyyymm'],format='%Y%m')+pd.offsets.MonthEnd(0)\n",
    "data_ma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233ddf4",
   "metadata": {},
   "source": [
    "### Construct the Dataset including all the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1bfd0",
   "metadata": {},
   "source": [
    "Besides adding the interaction terms, this function also transform the data into (-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbaaa1f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:12:24.619031Z",
     "start_time": "2024-06-17T09:12:23.091486Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def interactions(data_ch, data_ma, characteristics, ma_predictors, minmax=True):\n",
    "    # construct interactions between firm characteristics and macroeconomic predictors\n",
    "    data = data_ch.copy()\n",
    "    data_ma_long = pd.merge(data[['DATE']],data_ma,left_on='DATE',right_on='yyyymm',how='left')\n",
    "    data = data.reset_index(drop=True)\n",
    "    data_ma_long = data_ma_long.reset_index(drop=True)\n",
    "    for fc in characteristics:\n",
    "        for mp in ma_predictors:\n",
    "            data[fc+'*'+mp] = data[fc]*data_ma_long[mp]\n",
    "\n",
    "    features = list(set(data.columns).difference({'permno','DATE','RET'})) # a list storing all 920 features used\n",
    "    if minmax:\n",
    "        X = MinMaxScaler((-1,1)).fit_transform(data[features])\n",
    "        X = pd.DataFrame(X, columns=features)\n",
    "    else:\n",
    "        X = data[features]\n",
    "    y = data['RET']\n",
    "    print(f\"The shape of the data is: {data.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b91d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:32.559217Z",
     "start_time": "2024-06-17T09:00:32.559217Z"
    }
   },
   "outputs": [],
   "source": [
    "# type(data_ch[['DATE']])\n",
    "# type(data_ch['DATE'])\n",
    "# data_ma\n",
    "# data_ch.head()\n",
    "ma_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c2a5c",
   "metadata": {},
   "source": [
    "### Split the Sample into Training Set, Validation Set and Testing Set\n",
    "\n",
    "According to the paper, the authors use first 18 years (1957-1974) for training, last 30 years (1987-2016) for out-of-sample testing, and the 12 years in the middle (1975-1986) for tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f2bf0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:12:45.719790Z",
     "start_time": "2024-06-17T09:12:45.681028Z"
    }
   },
   "outputs": [],
   "source": [
    "#stdt_vld = np.datetime64('1975-01-31')\n",
    "#stdt_tst = np.datetime64('1987-01-31')\n",
    "stdt_vld = np.datetime64('2009-01-31')\n",
    "stdt_tst = np.datetime64('2015-01-31')\n",
    "\n",
    "def trn_vld_tst(data):\n",
    "\n",
    "    # training setstdt_vld = np.datetime64('2001-01-31')\n",
    "    X_trn, y_trn = interactions(data[data['DATE']<stdt_vld],data_ma[data_ma['yyyymm']<stdt_vld],characteristics,ma_predictors)\n",
    "\n",
    "    # validation set\n",
    "    X_vld, y_vld = interactions(data[(data['DATE']<stdt_tst)&(data['DATE']>=stdt_vld)],data_ma[(data_ma['yyyymm']<stdt_tst)&(data_ma['yyyymm']>=stdt_vld)],characteristics,ma_predictors)\n",
    "\n",
    "    # testing set\n",
    "    X_tst, y_tst = interactions(data[data['DATE']>=stdt_tst],data_ma[data_ma['yyyymm']>=stdt_tst],characteristics,ma_predictors)\n",
    "    return X_trn, X_vld, X_tst, y_trn, y_vld, y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dd2b54f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:13:13.762735Z",
     "start_time": "2024-06-17T09:12:48.660258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is: (96000, 915)\n",
      "The shape of the data is: (72000, 915)\n",
      "The shape of the data is: (72000, 915)\n",
      "CPU times: total: 10.2 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_trn, X_vld, X_tst, y_trn, y_vld, y_tst = trn_vld_tst(data_ch_top_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf861c",
   "metadata": {},
   "source": [
    "The differnce in the number of features results from the number of dummies from SIC code in the sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ba926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:00:32.568217Z",
     "start_time": "2024-06-17T09:00:32.568217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7a4aeba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:13:24.803301Z",
     "start_time": "2024-06-17T09:13:24.497493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2396"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a92d734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:13:26.986475Z",
     "start_time": "2024-06-17T09:13:26.604642Z"
    }
   },
   "outputs": [],
   "source": [
    "del([data_ch,data_ch_top,data_ch_bot,data_ch_d,data_ch_top_d,data_ch_bot_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4490a",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "In total, 8 models, including ordinary least squares (__OLS__), partial least squares (__PLS__), principal component regression (__PCR__), elastic net (__ENet__), generalized linear model with group lasso (__GLM__), random forest (__RF__), gradient boosting regression trees (__GBRT__) and neural networks (__NN__), are implemented in this paper. The objective loss functions are mean squared errors and Huber robust objective function that substitute squared loss with absolute loss for outliers. The training process is done in the training set, and the hyperparameters are tuned in the validation set.\n",
    "\n",
    "Due to the limitation of computational power, I mainly use the default or preselected hyperparameters when training the model. However, as the target of this replication is to get familiar with the whole process of implementing machine learning methods for empirical asset pricing research, I will conduct tuning process for some simple models (e.g., tuning the parameter $\\xi$ in Huber loss function when using preselected features including size, bm and momentum). Besides, as scikit-learn does not provide the flexibility to customize loss function in the training process, I will also write a customized code to incorporate Huber loss with elastic net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eaaa1",
   "metadata": {},
   "source": [
    "### Customized Loss Function, Scoring Functions, Validation Funtion and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff8f3ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:13:28.750403Z",
     "start_time": "2024-06-17T09:13:28.582929Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Loss Function\n",
    "# Huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# Scoring Function\n",
    "# out-of-sample R squared\n",
    "def R_oos(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    predicted = np.where(predicted<0,0,predicted)\n",
    "    return 1 - (np.dot((actual-predicted),(actual-predicted)))/(np.dot(actual,actual))\n",
    "\n",
    "# Validation Function\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True, sleep=0, is_NN=False):\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            time.sleep(sleep)\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n",
    "    \n",
    "\n",
    "# Pairwise Comparison\n",
    "# Diebold-Mariano test statistics\n",
    "\n",
    "# Evaluation Output\n",
    "def evaluate(actual, predicted, insample=False):\n",
    "    if insample:\n",
    "        print('*'*15+'In-Sample Metrics'+'*'*15)\n",
    "        print(f'The in-sample R2 is {r2_score(actual,predicted)*100:.2f}%')\n",
    "        print(f'The in-sample MSE is {mean_squared_error(actual,predicted):.3f}')\n",
    "    else:\n",
    "        print('*'*15+'Out-of-Sample Metrics'+'*'*15)\n",
    "        print(f'The out-of-sample R2 is {R_oos(actual,predicted)*100:.2f}%')\n",
    "        print(f'The out-of-sample MSE is {mean_squared_error(actual,predicted):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb531950",
   "metadata": {},
   "source": [
    "### OLS\n",
    "#### MSE as objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b98d30c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:27:52.050967Z",
     "start_time": "2024-06-17T07:27:30.583622Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 19.40%\n",
      "The in-sample MSE is 0.009\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -86.22%\n",
      "The out-of-sample MSE is 0.054\n",
      "CPU times: total: 22.6 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# OLS with all features\n",
    "OLS = LinearRegression().fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d331e058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:28:11.899483Z",
     "start_time": "2024-06-17T07:28:11.752441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.45%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.45%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# OLS with preselected size, bm, and momentum covariates\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)\n",
    "evaluate(y_trn, OLS_3.predict(X_trn[features_3]), insample=True)\n",
    "evaluate(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bc6c5c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:28:16.610524Z",
     "start_time": "2024-06-17T07:28:16.258398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2756"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a22e31",
   "metadata": {},
   "source": [
    "#### Huber objective fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14b019",
   "metadata": {},
   "source": [
    "The authors choose $\\xi$ as 99.9% percentile of the pricing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e633cc1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:32:23.460529Z",
     "start_time": "2024-06-17T07:32:23.411912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_trn-OLS.predict(X_trn)\n",
    "# (y_trn-OLS.predict(X_trn)).quantile(.999)\n",
    "# np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "# np.max(((y_trn-OLS_3.predict(X_trn[features_3])).quantile(.999),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1942c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:29:40.263920Z",
     "start_time": "2024-06-17T07:28:18.677166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 13.48%\n",
      "The in-sample MSE is 0.010\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -3.13%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: total: 1min 8s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# OLS by Huber robust objective function with all features\n",
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS_H.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS_H.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dae7ff21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:32:53.635680Z",
     "start_time": "2024-06-17T07:32:52.903577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.23%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.61%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 707 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# OLS by Huber robust objective function\n",
    "# with preselected size, bm, and momentum covariates\n",
    "epsilon = np.max(((y_trn-OLS_3.predict(X_trn[features_3])).quantile(.999),1))\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_H_3 = HuberRegressor(epsilon=epsilon).fit(X_trn[features_3],y_trn)\n",
    "\n",
    "evaluate(y_trn, OLS_H_3.predict(X_trn[features_3]), insample=True)\n",
    "evaluate(y_tst, OLS_H_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a67e490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:32:59.309123Z",
     "start_time": "2024-06-17T07:32:58.896086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4603"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42975a",
   "metadata": {},
   "source": [
    "### PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a7de9ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:34:44.104891Z",
     "start_time": "2024-06-17T07:33:24.180060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00032%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: -0.00010%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.00528%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: -2970.40320%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 1}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 1min 2s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c989fa3",
   "metadata": {},
   "source": [
    "As the result suggests, let's use only 1 component in PLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a7e461a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:35:13.877725Z",
     "start_time": "2024-06-17T07:35:12.665476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 5.70%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.02%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 500 ms\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pls_pred_is = PLS.predict(X_trn)\n",
    "pls_pred_os = PLS.predict(X_tst)\n",
    "evaluate(y_trn, pls_pred_is, insample=True) \n",
    "evaluate(y_tst, pls_pred_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cafc1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:35:21.856256Z",
     "start_time": "2024-06-17T07:35:21.409250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b874147",
   "metadata": {},
   "source": [
    "### PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b17e344c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:36:13.031353Z",
     "start_time": "2024-06-17T07:36:12.881541Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCRegressor:\n",
    "    \n",
    "    def __init__(self,n_PCs=1,loss='mse'):\n",
    "        self.n_PCs = n_PCs\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X = np.array(X)\n",
    "        N,K = X.shape\n",
    "        y = np.array(y_trn).reshape((N,1))\n",
    "        self.mu = np.mean(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.std(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.where(self.sigma==0,1,self.sigma)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pca = PCA()\n",
    "        X = pca.fit_transform(X)[:,:self.n_PCs]\n",
    "        self.pc_coef = pca.components_.T[:,:self.n_PCs]\n",
    "        if self.loss == 'mse':\n",
    "            self.model = LinearRegression().fit(X,y)\n",
    "        else:\n",
    "            self.model = HuberRegressor().fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = np.array(X)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        X = X @ self.pc_coef\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85c33525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:40:14.376005Z",
     "start_time": "2024-06-17T07:36:19.710740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.05091%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.02962%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 0.72772%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.04586%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.02467%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 2.06964%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'huber', 'n_PCs': 50}\n",
      "with R2oos 2.07% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 10min 2s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# principal component regression\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521476c",
   "metadata": {},
   "source": [
    "As the result suggests, let's use 3 principal components in PCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b074eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:40:23.320550Z",
     "start_time": "2024-06-17T07:40:22.882048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c710cbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:40:27.513756Z",
     "start_time": "2024-06-17T07:40:25.434438Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 6.88%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.01%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: total: 2.8 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, PCR.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, PCR.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c80a9a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:40:30.027849Z",
     "start_time": "2024-06-17T07:40:29.624479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfb907",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f01ea",
   "metadata": {},
   "source": [
    "Since scikit-learn does not provide the flexibility of choosing objective function, I code this part refering to the accelerated proximal algorithm (APG) documented in the online appendix. Besides, I see lasso and ridge as a special case of elastic net.\n",
    "\n",
    "I use the setting from the paper, where the regularized objective function is:\n",
    "$$L(\\theta;\\cdot) = L(\\theta) + \\phi(\\theta;\\cdot)$$\n",
    "where $\\theta$ is the vector of parameters, $L(\\theta)$ is the loss funciton (MSE or Huber loss in our case), and $\\phi(\\theta;\\cdot)$ is the penalty.\n",
    "\n",
    "Specifically, for our elastic net:\n",
    "$$\\phi(\\theta;\\lambda,\\rho)=\\lambda(1-\\rho)\\sum_{j=1}^{P}|\\theta_{j}|+\\frac{1}{2}\\lambda\\rho\\sum_{j=1}^{P}\\theta_{j}^{2}$$\n",
    "\n",
    "As for the details of APG, I refer the readers to the online appendix of the paper, which can be found on Prof Xiu's [website](https://dachxiu.chicagobooth.edu/). The parameter $\\gamma$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98d17530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:40:45.853117Z",
     "start_time": "2024-06-17T07:40:45.755806Z"
    }
   },
   "outputs": [],
   "source": [
    "# mse\n",
    "def mse(actual, predicted):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    return np.mean(resid**2)\n",
    "\n",
    "# huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(X, y, theta):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    return (X.T @ (y - X@theta))/N\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(X, y, theta, xi):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    return (grad_m+grad_u+grad_l)/N\n",
    "\n",
    "# proximal operator\n",
    "def prox(theta,lmd,rho,gamma):\n",
    "    return (1/(1+lmd*gamma*rho))*softhred(theta,(1-rho)*gamma*lmd)\n",
    "\n",
    "# soft-thresholding operator\n",
    "def softhred(x,mu):\n",
    "    x = np.where(np.abs(x)<=mu, 0, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x>0), x-mu, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x<0), x+mu, x)\n",
    "    return x\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd=1, rho=0.5, L=1, verbose=False,\n",
    "        xi=1.35, max_iter=3000, tol=1e-4, loss='huber', random_state=None\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.L = L\n",
    "        self.verbose = verbose\n",
    "        self.xi = xi\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N,1))\n",
    "        gamma = 1/self.L\n",
    "        # initialize theta\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(size=(K,1))\n",
    "        else:\n",
    "            theta = np.zeros((K,1))\n",
    "        \n",
    "        for m in np.arange(self.max_iter):\n",
    "            theta_old = theta\n",
    "            \n",
    "            if self.loss == 'mse':\n",
    "                theta_bar = theta - gamma*grad_mse(X,y,theta)\n",
    "            else:\n",
    "                theta_bar = theta - gamma*grad_huber(X,y,theta,self.xi)\n",
    "                \n",
    "            theta_til = prox(theta_bar,self.lmd,self.rho,gamma)\n",
    "            theta = theta_til + m/(m+3)*(theta_til-theta)\n",
    "            gamma = gamma\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'{m+1} iters finished.')\n",
    "                print(f'theta = {theta.T}')\n",
    "                print(f'{np.sum((theta-theta_old)**2)}')\n",
    "            \n",
    "            if np.sum((theta-theta_old)**2)<np.sum(theta_old**2*self.tol) or np.sum(np.abs(theta-theta_old))==0:\n",
    "                break\n",
    "        self.theta = theta_old\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc9cae43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:41:46.852912Z",
     "start_time": "2024-06-17T07:40:59.043863Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'L': 43030456.77205997, 'lmd': 0.0001, 'loss': 'mse'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 43030456.77205997, 'lmd': 0.1, 'loss': 'mse'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'L': 43030456.77205997, 'lmd': 0.0001, 'loss': 'mse'}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 57.7 s\n",
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# compute L\n",
    "L = np.max(svd(X_trn,compute_uv=False))**2\n",
    "\n",
    "params = {\n",
    "    'lmd':[1e-4,.1],\n",
    "    'L':[L],\n",
    "    'loss':['mse']\n",
    "}\n",
    "\n",
    "EN_my_mse = val_fun(ENet,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c32292bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:41:54.148864Z",
     "start_time": "2024-06-17T07:41:53.547268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.03%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.00%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 589 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_mse.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, EN_my_mse.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9900438d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:41:57.237695Z",
     "start_time": "2024-06-17T07:41:56.812864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd0cb0f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:42:54.774684Z",
     "start_time": "2024-06-17T07:42:11.838496Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'L': 2.0, 'lmd': 0.0001, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.00019999999999999998, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0003, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.00039999999999999996, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0005, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0006000000000000001, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0007, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0007999999999999999, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.0009, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'L': 2.0, 'lmd': 0.001, 'loss': 'huber'} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'L': 2.0, 'lmd': 0.0001, 'loss': 'huber'}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 29.6 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xi = np.max((1,(y_trn-EN_my_mse.predict(X_trn).flatten()).quantile(.999)))\n",
    "params = {\n",
    "    'lmd':list(np.linspace(1e-4,1e-3,10)),\n",
    "    'L':[2*xi],\n",
    "    #'L':[2],\n",
    "    'loss':['huber']\n",
    "}\n",
    "EN_my_hub = val_fun(ENet,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f4e2790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:50:00.071237Z",
     "start_time": "2024-06-17T07:49:59.642222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d245d50",
   "metadata": {},
   "source": [
    "However, the choice of learning rate $\\gamma$ in APG is quite cruicial. According to [Parikh and Boyd (2013)](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf), the algorithm converges when $\\gamma \\in (0, 1/L]$, where $L$ is the _Lipschitz_ constant of the gradient of the objective funtion. As for mse, L is the square of the largest singular value of the dataset. Though my code for APG seems to work, it returns weights with same value. The readers can check this by setting the parameter _verbose_ as _False_. Therefore, I first use the elastic net provided by sklearn for the elastic net with MSE as objective function. Then, I write the fitting process as an unconstraint optimization using the optimization package from scipy. However, as I use Nelder-Mead as solver, it takes much time to get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3392e9f",
   "metadata": {},
   "source": [
    "__Elastic Net from sklearn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29f4eba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:52:51.801854Z",
     "start_time": "2024-06-17T07:50:09.352894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'alpha': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -83.97803%\n",
      "************************************************************\n",
      "Model with params: {'alpha': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.62828%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'alpha': 0.1}\n",
      "with R2oos 0.63% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 1min 7s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "params = {'alpha':[1e-4,.1]}\n",
    "EN_sk = val_fun(ElasticNet,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8363664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:52:56.906511Z",
     "start_time": "2024-06-17T07:52:56.459370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.00%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.35%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 435 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_sk.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, EN_sk.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703965a",
   "metadata": {},
   "source": [
    "__Elastic Net with Huber Loss by Scipy__\n",
    "\n",
    "Next, I formulate this problem in the framework of scipy's optimization package. Though BFGS solver uses gradient and will be faster than Nelder-Mead, I encounter a runtime error _'Desired error not necessarily achieved due to precision loss'_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8597bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:53:01.797447Z",
     "start_time": "2024-06-17T07:53:01.662929Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "# penalized mse\n",
    "def mse_pnl(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    return np.mean(resid**2) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# penalized huber objective function\n",
    "def huber_pnl(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, resid**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    grad = (X.T @ (y - X@theta))/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    grad = (grad_m+grad_u+grad_l)/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd, rho, xi=1.35, loss='huber', random_state=None, fit_intercept=True\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.xi = xi\n",
    "        self.random_state = random_state\n",
    "        self.fit_intercept = fit_intercept\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N,1))\n",
    "        if self.fit_intercept:\n",
    "            K += 1\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        # initialize theta\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(K)\n",
    "        else:\n",
    "            theta = np.zeros(K)\n",
    "        \n",
    "        if self.loss == 'huber':\n",
    "            res = minimize(\n",
    "                partial(huber_pnl, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho), theta,\n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_huber, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        else:\n",
    "            res = minimize(\n",
    "                partial(mse_pnl, X=X, y=y, lmd=self.lmd, rho=self.rho), theta, \n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_mse, X=X, y=y, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        \n",
    "        self.theta = res.x.reshape((K,1))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        N = X.shape[0]\n",
    "        if self.fit_intercept:\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8c331",
   "metadata": {},
   "source": [
    "As the Nelder-Mead solver is really slow, I randomly select 50 features to try whther my code works. I can foresee that my code will be much slower than sklearn's elastic net. Besides, as the solvers are different and the stop criteria are different, the results might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d0d8e67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:53:29.521028Z",
     "start_time": "2024-06-17T07:53:03.646141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011509\n",
      "         Iterations: 672\n",
      "         Function evaluations: 921\n",
      "CPU times: total: 5.2 s\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.00025,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "import random\n",
    "\n",
    "# randomly select 50 \n",
    "random.seed(12308)\n",
    "features_rdm = random.sample(list(X_trn.columns),50)\n",
    "\n",
    "# fit elastic net with mse using my code\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='mse').fit(X_trn[features_rdm],y_trn)\n",
    "# the fitted coefficients\n",
    "EN_my_mse_rdm.theta.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c14f5ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:53:39.520047Z",
     "start_time": "2024-06-17T07:53:39.508011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['std_turn*ep_sp', 'pricedelay*dfy', 'invest*dfy',\n",
       "       'pchsale_pchrect*ntis', 'pchcurrat', 'beta*dp_sp', 'ps*tbl',\n",
       "       'sic_46', 'agr*ntis', 'salecash*dfy', 'pchsale_pchxsga',\n",
       "       'chmom*bm_sp', 'mom12m*bm_sp', 'roic*dfy', 'sic_14', 'acc*dp_sp',\n",
       "       'divo*tms', 'acc*bm_sp', 'sic_23', 'sic_20', 'retvol', 'turn',\n",
       "       'hire*tms', 'sic_29', 'cinvest*tms', 'stdcf*tbl', 'sic_39',\n",
       "       'convind*dfy', 'acc*tbl', 'aeavol*tms', 'orgcap*bm_sp',\n",
       "       'roaq*svar', 'maxret*dfy', 'dolvol*ntis', 'chmom*ep_sp', 'sic_35',\n",
       "       'pchcapx_ia*tbl', 'mve_ia', 'pchsaleinv*bm_sp', 'bm*svar',\n",
       "       'ps*svar', 'pchsaleinv*ntis', 'tb*ntis', 'realestate*dp_sp',\n",
       "       'age*tms', 'invest*tms', 'salerec*tbl', 'salerec*dfy', 'sic_12',\n",
       "       'divi*tbl'], dtype='<U20')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 50 features selected\n",
    "np.array(features_rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ca556c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:53:43.542864Z",
     "start_time": "2024-06-17T07:53:43.441675Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.00%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.01%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 90.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_mse_rdm.predict(X_trn[features_rdm]), insample=True) \n",
    "evaluate(y_tst, EN_my_mse_rdm.predict(X_tst[features_rdm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e70ba3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:53:49.602819Z",
     "start_time": "2024-06-17T07:53:49.390086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 189 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00121906, -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.00208623,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit elastic net with mse by sklearn\n",
    "EN_sk_rdm = ElasticNet(alpha=.01).fit(X_trn[features_rdm],y_trn)\n",
    "# fitted coefficients\n",
    "np.array([EN_sk_rdm.intercept_]+list(EN_sk_rdm.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe2b3828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:53:52.364547Z",
     "start_time": "2024-06-17T07:53:52.295546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.19%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.26%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 60.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_sk_rdm.predict(X_trn[features_rdm]), insample=True) \n",
    "evaluate(y_tst, EN_sk_rdm.predict(X_tst[features_rdm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864a4e9",
   "metadata": {},
   "source": [
    "Though I have foreseen that the results might be different between my code and sklearn, the difference is beyond my expectation. Anyway, let's try my code with huber loss. We should expect some improvement compared with mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc4bb15e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:54:21.166215Z",
     "start_time": "2024-06-17T07:53:55.322769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011508\n",
      "         Iterations: 667\n",
      "         Function evaluations: 921\n",
      "CPU times: total: 6.03 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "\n",
    "# fit elastic net with huber loss using my code\n",
    "EN_my_hub_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn[features_rdm],y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c42e69e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:54:39.651425Z",
     "start_time": "2024-06-17T07:54:39.454041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.00%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.01%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_hub_rdm.predict(X_trn[features_rdm]), insample=True) \n",
    "evaluate(y_tst, EN_my_hub_rdm.predict(X_tst[features_rdm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6874a33",
   "metadata": {},
   "source": [
    "Fortunately, there is indeed some improvement with regard to $R^{2}_{oos}$ as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025cb4a0",
   "metadata": {},
   "source": [
    "### GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24e32444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:55:59.744231Z",
     "start_time": "2024-06-17T07:55:51.918581Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install group_lasso -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa1788aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T07:56:34.377235Z",
     "start_time": "2024-06-17T07:56:34.306426Z"
    }
   },
   "outputs": [],
   "source": [
    "from group_lasso import GroupLasso\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def SplineTransform(data,knots=3):\n",
    "    spline_data = pd.DataFrame(np.ones((data.shape[0],1)),index=data.index,columns=['const'])\n",
    "    for i in data.columns:\n",
    "        i_dat = data.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(knots):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,knots+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "    return spline_data\n",
    "\n",
    "class GLMRegression:\n",
    "    \n",
    "    def __init__(self,knots=3,lmd=1e-4,l1_reg=1e-4,random_state=12308):\n",
    "        self.knots = knots\n",
    "        self.lmd = lmd\n",
    "        self.random_state = random_state\n",
    "        self.l1_reg = l1_reg\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        groups = [0]+flatten([list(np.repeat(i,self.knots+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        X = SplineTransform(X)\n",
    "        self.mod = GroupLasso(\n",
    "            groups=groups,group_reg=self.lmd,l1_reg=self.l1_reg,\n",
    "            fit_intercept=False,random_state=self.random_state\n",
    "        )\n",
    "        self.mod = self.mod.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = SplineTransform(X)\n",
    "        return self.mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbf47d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:13:27.250988Z",
     "start_time": "2024-06-17T08:04:43.743673Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:6\u001b[0m\n",
      "Cell \u001b[1;32mIn[40], line 28\u001b[0m, in \u001b[0;36mval_fun\u001b[1;34m(model, params, X_trn, y_trn, X_vld, y_vld, illustration, sleep, is_NN)\u001b[0m\n\u001b[0;32m     26\u001b[0m     mod \u001b[38;5;241m=\u001b[39m model()\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\u001b[38;5;241m.\u001b[39mfit(X_trn, y_trn, X_vld, y_vld)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m best_mod \u001b[38;5;241m=\u001b[39m mod\n\u001b[0;32m     30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mpredict(X_vld)\n",
      "Cell \u001b[1;32mIn[80], line 34\u001b[0m, in \u001b[0;36mGLMRegression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,y):\n\u001b[0;32m     33\u001b[0m     groups \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mflatten([\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mrepeat(i,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknots\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))[:] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m---> 34\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mSplineTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmod \u001b[38;5;241m=\u001b[39m GroupLasso(\n\u001b[0;32m     36\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,group_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlmd,l1_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg,\n\u001b[0;32m     37\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmod\u001b[38;5;241m.\u001b[39mfit(X,y)\n",
      "Cell \u001b[1;32mIn[80], line 16\u001b[0m, in \u001b[0;36mSplineTransform\u001b[1;34m(data, knots)\u001b[0m\n\u001b[0;32m     14\u001b[0m         i_dum\u001b[38;5;241m.\u001b[39miloc[:,j] \u001b[38;5;241m=\u001b[39m i_dum\u001b[38;5;241m.\u001b[39miloc[:,j]\u001b[38;5;241m*\u001b[39m((i_dat\u001b[38;5;241m-\u001b[39mbins[j])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     15\u001b[0m     i_dum\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,knots\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m---> 16\u001b[0m     spline_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspline_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi_dat\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi_dum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spline_data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m new_values[argsort]\n\u001b[0;32m   2302\u001b[0m     new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[1;32m-> 2304\u001b[0m     bp \u001b[38;5;241m=\u001b[39m \u001b[43mBlockPlacement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement\u001b[38;5;241m=\u001b[39mbp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6d760db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(GLM.mod.sparsity_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f7262cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:13:32.441873Z",
     "start_time": "2024-06-17T08:13:31.274837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14626"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "017a1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 3.01%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.73%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 2min 19s, sys: 21.4 s, total: 2min 40s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, GLM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, GLM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a74a0ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:13:36.743035Z",
     "start_time": "2024-06-17T08:13:36.305763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1367"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabced51",
   "metadata": {},
   "source": [
    "### Tree-based Models\n",
    "\n",
    "In this notebook, the best models are tree-based models, including LGBM, Random Forest and XGBoost. Quoted from the [first place solution](https://www.kaggle.com/competitions/ubiquant-market-prediction/discussion/338220) for [Ubiquant Market Prediction Competition](https://www.kaggle.com/competitions/ubiquant-market-prediction/overview): LGBM is a powerful model whose performance has been proven in many competitions. It was also the most stable (especially the consistency of CV and LB) and excellent in the experiment on the competition data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b95a29",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "From this section till XGBoost, we will explore some tree-based models. We skip the single decision tree, as it is highly sensitive to the dataset. We first try gradient boosting decision tree (GBDT). The rationale behind is that several weak learners together result in a better performance than a single sophisticated learner. I use LightGBM to accelerate the training process while reserve the accuracy.\n",
    "\n",
    "[LightGBM](https://lightgbm.readthedocs.io/en/v3.3.3/index.html) is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "- Faster training speed and higher efficiency.\n",
    "\n",
    "- Lower memory usage.\n",
    "\n",
    "- Better accuracy.\n",
    "\n",
    "- Support of parallel, distributed, and GPU learning.\n",
    "\n",
    "- Capable of handling large-scale data.\n",
    "\n",
    "LightGBM speeds up the training process of convenional gradient boosting decision tree by up to over 20 times while achieving almost the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a568a3f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:16:14.847924Z",
     "start_time": "2024-06-17T08:16:14.809562Z"
    }
   },
   "outputs": [],
   "source": [
    "# huber loss function for customized objective function\n",
    "\n",
    "# gradient of huber loss with respect to y_pred\n",
    "def grad_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35 \n",
    "    # Though I do not want to make it hard-coded, lightgbm, behind the scene, evaluates the # of parameters\n",
    "    # of the objective function first, then pass according # of parameters. I tried to use partial to set \n",
    "    # the value of xi. It did not work.\n",
    "    # I refer the readers to the source code to have a better understanding of the issue:\n",
    "    # (https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/sklearn.py)\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    grad = np.zeros(N)\n",
    "    try:\n",
    "        grad[ind_m] = (-2*(y_true-y_pred))[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_u] = np.repeat(2*xi,N)[ind_u]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_l] = np.repeat(-2*xi,N)[ind_l]\n",
    "    except:\n",
    "        pass\n",
    "    return grad/N\n",
    "\n",
    "# hessian of huber loss with respect to y_pred\n",
    "def hess_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    hess = np.zeros(N)\n",
    "    try:\n",
    "        hess[ind_m] = np.repeat(2,N)[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    return hess/N\n",
    "\n",
    "# huber loss for lgbm\n",
    "def huber_obj(y_true, y_pred):\n",
    "    grad = grad_huber_obj(y_true, y_pred)\n",
    "    hess = hess_huber_obj(y_true, y_pred)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b40e07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:28:52.308189Z",
     "start_time": "2024-06-17T08:16:16.992443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.823502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.87070%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.945823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.23432%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.699334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.92921%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.718471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.51671%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.790583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.09914%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.748179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.01141%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.710423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 200, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.34051%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.768633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 200, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.33230%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.696749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.56229%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.783563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.50883%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.772891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.05352%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.799727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.09402%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.710242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 10, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.57218%\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.697420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 10, 'objective': <function huber_obj at 0x0000023F1B0E5870>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.26757%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.732714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Start training from score 0.001823\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.93968%\n",
      "************************************************************\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.742374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205596\n",
      "[LightGBM] [Info] Number of data points in the train set: 96000, number of used features: 909\n",
      "[LightGBM] [Info] Using self-defined objective functi"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10003 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[10,50,100,200,500,1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(LGBMRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33a07592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd4087b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 19.41%\n",
      "The in-sample MSE is 0.009\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.26%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: user 2.29 s, sys: 898 ms, total: 3.19 s\n",
      "Wall time: 944 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, LGBM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18370f24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:40:06.442269Z",
     "start_time": "2024-06-17T08:40:06.295488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c47f15",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3106c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:54:20.113596Z",
     "start_time": "2024-06-17T08:40:08.157194Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00123%\n",
      "************************************************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:9\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 42\u001b[0m, in \u001b[0;36mval_fun\u001b[1;34m(model, params, X_trn, y_trn, X_vld, y_vld, illustration, sleep, is_NN)\u001b[0m\n\u001b[0;32m     40\u001b[0m     mod \u001b[38;5;241m=\u001b[39m model()\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\u001b[38;5;241m.\u001b[39mfit(X_trn, y_trn, X_vld, y_vld)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mpredict(X_vld)\n\u001b[0;32m     44\u001b[0m ros \u001b[38;5;241m=\u001b[39m R_oos(y_vld, y_pred)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21baaaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "381e8488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 19.96%\n",
      "The in-sample MSE is 0.009\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 2.29%\n",
      "The out-of-sample MSE is 0.008\n",
      "CPU times: user 1.86 s, sys: 276 ms, total: 2.14 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "\n",
    "evaluate(y_trn, RF.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, RF.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c33e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:54:25.044602Z",
     "start_time": "2024-06-17T08:54:24.833255Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2231a7",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Typically, the winner models of most Kaggle competitions are XGBoost, Neural Nets, or the ensemble model of these two. Therefore, I give XGBoost a try here, though it is not included in the paper. Like other additive tree-based models, XGBoost model often achieves higher accuracy than a single decision tree. However, it sacrifices the intrinsic interpretability of decision trees. Typically, it is easy to encounter overfitting issue when using XGBoost, especially in a low signal-to-noise ratio context. Therefore, here I try some simple settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78ff535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.50783%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 600, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.43211%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.33577%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.02993%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.59823%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 600, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.21747%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.08021%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.10506%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'random_state': 12308}\n",
      "with R2oos 2.60% on validation set.\n",
      "############################################################\n",
      "CPU times: user 4h 12min 3s, sys: 2min 21s, total: 4h 14min 24s\n",
      "Wall time: 33min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [500,600,800,1000],\n",
    "    'max_depth': [1,2],\n",
    "    'random_state': [12308],\n",
    "    'learning_rate': [.01]\n",
    "}\n",
    "XGB = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1485d003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "006affbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 14.87%\n",
      "The in-sample MSE is 0.010\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.50%\n",
      "The out-of-sample MSE is 0.008\n",
      "CPU times: user 9.41 s, sys: 35 ms, total: 9.45 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, XGB.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, XGB.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8208d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830e2d4",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97852cb0",
   "metadata": {},
   "source": [
    "Next, we consider Neural Networks. As neural nets are highly parameterised, it is easy to overfit. I use the regularization methods mentioned in the paper, say _learning rate shrinkage_ (incorporated in Adam solver), _early stopping_, _batch normaliztion_ and _ensembles_. Besides, another requirement from so many parameters is more data. Therefore, if you use tiny dataset to have a taste of neural nets, it is very likely that it underperform simpler models. However, unlike the paper, my NN5 has best out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e0a784f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:14:17.030850Z",
     "start_time": "2024-06-17T09:14:16.960862Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# customized metrics\n",
    "# out-of-sample r squared for keras\n",
    "def R_oos_tf(y_true, y_pred):\n",
    "    resid = tf.square(y_true-y_pred)\n",
    "    denom = tf.square(y_true)\n",
    "    return 1 - tf.divide(tf.reduce_mean(resid),tf.reduce_mean(denom))\n",
    "\n",
    "# data standardization\n",
    "# please standardize the data if BatchNormalization is not used\n",
    "def standardize(X_trn, X_vld, X_tst):\n",
    "    mu_trn = np.mean(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "    sigma_trn = np.std(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "\n",
    "    X_trn_std = (np.array(X_trn)-mu_trn)/sigma_trn\n",
    "    X_vld_std = (np.array(X_vld)-mu_trn)/sigma_trn\n",
    "    X_tst_std = (np.array(X_tst)-mu_trn)/sigma_trn\n",
    "    return X_trn_std, X_vld_std, X_tst_std\n",
    "\n",
    "# NN class\n",
    "class NN:\n",
    "    \n",
    "    def __init__(\n",
    "        self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, BatchNormalization=True, patience=5,\n",
    "        epochs=100, batch_size=3000, verbose=1, random_state=12308, monitor='val_R_oos_tf', base_neurons=5\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.learning_rate = learning_rate\n",
    "        self.BatchNormalization = BatchNormalization\n",
    "        self.patience = patience\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.monitor = monitor\n",
    "        self.base_neurons = base_neurons\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_trn, y_trn, X_vld, y_vld):\n",
    "        # fix random seed for reproducibility\n",
    "        random.seed(self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        tf.random.set_seed(self.random_state)\n",
    "        \n",
    "        # model construction\n",
    "        mod = Sequential()\n",
    "        mod.add(Input(shape=(X_trn.shape[1],)))\n",
    "        \n",
    "        for i in np.arange(self.n_layers,0,-1):\n",
    "            if self.n_layers>self.base_neurons:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**i, activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**i, activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            else:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), \n",
    "                                  activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            if self.BatchNormalization:\n",
    "                mod.add(BatchNormalization())\n",
    "        \n",
    "        mod.add(Dense(1, kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "        \n",
    "        # early stopping\n",
    "        earlystop = tf.keras.callbacks.EarlyStopping(monitor=self.monitor, patience=self.patience)\n",
    "\n",
    "        # Adam solver\n",
    "        opt = Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # compile the model\n",
    "        mod.compile(loss=self.loss,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[R_oos_tf])\n",
    "\n",
    "        # fit the model\n",
    "        mod.fit(X_trn, np.array(y_trn).reshape((len(y_trn),1)), epochs=self.epochs, batch_size=self.batch_size, \n",
    "                callbacks=[earlystop], verbose=self.verbose, \n",
    "                validation_data=(X_vld,np.array(y_vld).reshape((len(y_vld),1))))\n",
    "        \n",
    "        self.model = mod\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "196725d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:16:55.521865Z",
     "start_time": "2024-06-17T09:14:29.550025Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -955.62687%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -173.04665%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.03589%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.07696%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -128.63443%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -472.65738%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00746%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00030%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.08% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 1min 4s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8389f630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:17:34.993579Z",
     "start_time": "2024-06-17T09:17:34.418836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9120"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a14ac67a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:17:48.801624Z",
     "start_time": "2024-06-17T09:17:36.765664Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -83.50%\n",
      "The in-sample MSE is 0.021\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -137.97%\n",
      "The out-of-sample MSE is 0.020\n",
      "CPU times: total: 2.66 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN1.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN1.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72c098ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:17:54.707072Z",
     "start_time": "2024-06-17T09:17:54.360056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2819"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99994a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:20:52.592647Z",
     "start_time": "2024-06-17T09:17:58.643295Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -976.59937%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1895.05006%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -6.27205%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1188.69239%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -211.19191%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -982.64117%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -10.26166%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 1min 5s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7aea326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:20:58.607592Z",
     "start_time": "2024-06-17T09:20:57.917627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11852"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa3c9eff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:21:12.499895Z",
     "start_time": "2024-06-17T09:21:00.668449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -327.44%\n",
      "The in-sample MSE is 0.049\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -1.62%\n",
      "The out-of-sample MSE is 0.053\n",
      "CPU times: total: 3.16 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN2.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN2.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de107b0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:21:17.516225Z",
     "start_time": "2024-06-17T09:21:17.070476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2819"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c52a69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:24:25.622683Z",
     "start_time": "2024-06-17T09:21:28.773823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.94389%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -29.23308%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00215%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.53304%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.27631%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -98.23926%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.29700%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.19793%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 1.20% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 58.9 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc7b4448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:24:31.278764Z",
     "start_time": "2024-06-17T09:24:30.585467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17482"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "599a367c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:24:45.066105Z",
     "start_time": "2024-06-17T09:24:32.693269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.52%\n",
      "The in-sample MSE is 0.011\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.88%\n",
      "The out-of-sample MSE is 0.008\n",
      "CPU times: total: 3.39 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN3.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN3.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "572be9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:24:52.890182Z",
     "start_time": "2024-06-17T09:24:52.609420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2819"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "862a0c21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:28:13.178225Z",
     "start_time": "2024-06-17T09:25:03.293643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -25.83945%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -13.66331%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -12.69601%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -6.20974%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -3.84553%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -31.14478%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00005%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.07010%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.07% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 1min 3s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc3f0a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:28:19.301735Z",
     "start_time": "2024-06-17T09:28:18.649402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18647"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e24e1772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:28:32.669604Z",
     "start_time": "2024-06-17T09:28:20.808093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -32.49%\n",
      "The in-sample MSE is 0.015\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.26%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: total: 2.97 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN4.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN4.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc206184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "750bc124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:32:34.233226Z",
     "start_time": "2024-06-17T09:28:37.743828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.17801%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.85425%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.19844%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.67426%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -6771.18948%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.10372%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.20256%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.03159%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 1.67% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 1min 20s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f2cd08e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:32:42.529593Z",
     "start_time": "2024-06-17T09:32:41.681411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21961"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0fc5e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T09:32:57.747842Z",
     "start_time": "2024-06-17T09:32:43.356884Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -8.53%\n",
      "The in-sample MSE is 0.012\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.94%\n",
      "The out-of-sample MSE is 0.009\n",
      "CPU times: total: 3.02 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN5.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cc98952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8277ca18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T10:16:34.046900Z",
     "start_time": "2024-06-17T10:16:02.395162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 20.4750 - R_oos_tf: -1700.1594 - val_loss: 613.7759 - val_R_oos_tf: -79952.3281\n",
      "Epoch 2/100\n",
      " 7/50 [===>..........................] - ETA: 3s - loss: 1.1888 - R_oos_tf: -8.582"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10032 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Wide-NN1-Regression-[1024(relu)-1(linear)]\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "c = 10\n",
    "l1 = 1e-2\n",
    "l2 = 1e-2\n",
    "monitor = 'val_R_oos_tf'\n",
    "patience = 5\n",
    "learning_rate = 1e-1\n",
    "epochs = 100\n",
    "batch_size = int(X_trn.shape[0]/50)\n",
    "verbose = 1\n",
    "random_state = 12308\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "NN1W = Sequential()\n",
    "NN1W.add(Input(X_trn.shape[1]))\n",
    "NN1W.add(Dense(2**c, activation='relu'))\n",
    "NN1W.add(BatchNormalization())\n",
    "NN1W.add(Dense(1, kernel_regularizer=L1L2(l1,l2)))\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience)\n",
    "\n",
    "opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# compile the model\n",
    "NN1W.compile(loss='mse',\n",
    "             optimizer=opt,\n",
    "             metrics=[R_oos_tf])\n",
    "\n",
    "# fit the model\n",
    "NN1W.fit(X_trn, np.array(y_trn).reshape((len(y_trn),1)), epochs=epochs, batch_size=batch_size,\n",
    "         callbacks=[earlystop], \n",
    "         verbose=verbose,\n",
    "         validation_data=(X_vld,np.array(y_vld).reshape((len(y_vld),1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2850083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fda9cc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -1742.27%\n",
      "The in-sample MSE is 0.212\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -55.03%\n",
      "The out-of-sample MSE is 0.013\n",
      "CPU times: user 15.7 s, sys: 612 ms, total: 16.3 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN1W.predict(X_trn,verbose=0), insample=True) \n",
    "evaluate(y_tst, NN1W.predict(X_tst,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40d17084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1284"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b65225fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -129.40485%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -37.60405%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.17058%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.02012%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -4620.50051%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -422.96879%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.84491%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -2.57341%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1920, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 6, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.84% on validation set.\n",
      "############################################################\n",
      "CPU times: user 3min 16s, sys: 16.4 s, total: 3min 33s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN6-Regression-[64(relu)-32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [6],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN6 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8412ba38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22386"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5da985e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 10.72%\n",
      "The in-sample MSE is 0.010\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.74%\n",
      "The out-of-sample MSE is 0.010\n",
      "CPU times: user 5.86 s, sys: 792 ms, total: 6.65 s\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, NN6.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, NN6.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3aad59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49de0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "machlen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
